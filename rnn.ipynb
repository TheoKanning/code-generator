{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Generator RNN\n",
    "\n",
    "Writing code is hard, so let's make a character-level RNN do it for us\n",
    "But generating code is also hard, so start with tinyshakespeare first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for data collection/processing\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "print(f\"Downloaded shakespeare data to {path_to_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "1. Grab all java/groovy filenames\n",
    "2. Convert files into one big string\n",
    "3. Tokenize text\n",
    "4. Split string into samples\n",
    "4. Convert samples to one-hot vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(filename):\n",
    "    with open(filename) as f:\n",
    "        return f.read()\n",
    "\n",
    "# splits long text into training samples with a specified step size. \n",
    "# Returns list of samples and next characters\n",
    "def split_into_samples(text, sample_length, step):\n",
    "    samples = []\n",
    "    next_chars = []\n",
    "    \n",
    "    for i in range(0, len(text) - sample_length, step):\n",
    "        samples.append(text[i:i + sample_length])\n",
    "        next_chars.append(text[i + sample_length])\n",
    "    \n",
    "    mapIndexPosition = list(zip(samples, next_chars))\n",
    "    random.shuffle(mapIndexPosition)\n",
    "    samples, next_chars = zip(*mapIndexPosition)\n",
    "    \n",
    "    return samples, next_chars\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "text = get_text(path_to_file)\n",
    "print(f\"Total text length = {len(text)}\")\n",
    "\n",
    "tokenizer = Tokenizer(lower=False, char_level=True)\n",
    "tokenizer.fit_on_texts([text])\n",
    "vocab_size = len(tokenizer.word_index) + 1 # add one because the tokenizer reserves 0 for unknown characters\n",
    "print(f\"{vocab_size} characters in vocabulary\")\n",
    "\n",
    "sample_length = 100 # cut text into uniform sample length\n",
    "samples, next_chars = split_into_samples(text, sample_length, 10)\n",
    "num_samples = len(samples)\n",
    "print(f\"{num_samples} total samples\")\n",
    "print(f\"{num_samples * sample_length} total training characters\")\n",
    "\n",
    "samples = tokenizer.texts_to_sequences(samples)\n",
    "next_chars = tokenizer.texts_to_sequences(next_chars)\n",
    "\n",
    "# (num_samples, sample_length, vocab_size), (num_samples, vocab_size)\n",
    "X = to_categorical(samples, num_classes=vocab_size)\n",
    "Y = to_categorical(next_chars, num_classes=vocab_size)\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"Y shape: {Y.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Model\n",
    "\n",
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(sample_length, vocab_size):\n",
    "    \n",
    "    input_layer = Input(shape=(sample_length, vocab_size,))\n",
    "    m = LSTM(256, dropout=0.5, recurrent_dropout=0.5, return_sequences=True)(input_layer)\n",
    "    m = LSTM(256, dropout=0.5, recurrent_dropout=0.5)(m)\n",
    "    m = Dense(vocab_size, activation='softmax')(m)\n",
    "    \n",
    "    model = Model(inputs=[input_layer], outputs=m)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def plots(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    \n",
    "    plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(sample_length, vocab_size)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X, Y, batch_size=100, epochs=5, validation_split=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temperature sampling based on Deep Learning book\n",
    "def sample(preds, temperature):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probs = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probs)\n",
    "\n",
    "def generate(seed_text, length, temperature=0.2):\n",
    "    generated_text = seed_text\n",
    "    seed_text = seed_text.rjust(sample_length, ' ')\n",
    "    seed_text = seed_text[:sample_length]\n",
    "    \n",
    "    for i in range(0, length):\n",
    "        sampled = tokenizer.texts_to_matrix(seed_text)\n",
    "        sampled = np.reshape(sampled, (1, sample_length, vocab_size))\n",
    "\n",
    "        preds = model.predict(sampled)[0]\n",
    "        next_index = sample(preds, temperature)\n",
    "        next_char = ' '\n",
    "        \n",
    "        for char, index in tokenizer.word_index.items():\n",
    "            if index == next_index:\n",
    "                next_char = char\n",
    "        \n",
    "        generated_text += next_char\n",
    "        seed_text += next_char\n",
    "        seed_text = seed_text[1:] # keep seed text the same length\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = generate(\"A\", 100, temperature=0.5)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
